<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://nmashalov.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nmashalov.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-02-21T17:04:17+00:00</updated><id>https://nmashalov.github.io/feed.xml</id><title type="html">Nikita Mashalov</title><subtitle>The personal site of Nikita Mashalov. </subtitle><entry><title type="html"></title><link href="https://nmashalov.github.io/blog/2024/2024-01-08-three_d_model/" rel="alternate" type="text/html" title=""/><published>2024-02-21T17:04:17+00:00</published><updated>2024-02-21T17:04:17+00:00</updated><id>https://nmashalov.github.io/blog/2024/2024-01-08-three_d_model</id><content type="html" xml:base="https://nmashalov.github.io/blog/2024/2024-01-08-three_d_model/"><![CDATA[<p>Here’s my overview of current achievements in 3D</p> <p>Formats can be transformed</p> <h2 id="two-main-approaches">Two main approaches</h2> <table> <thead> <tr> <th>Flavor</th> <th>Distillation from 2d images</th> <th>Work with 3d models</th> </tr> </thead> <tbody> <tr> <td>Data</td> <td>Cheap</td> <td>Expensive</td> </tr> <tr> <td>Speed</td> <td>currently slow</td> <td>fast</td> </tr> <tr> <td>Influential works</td> <td><a href="https://arxiv.org/pdf/2305.16213.pdf">Profilic Dreamer</a></td> <td><a href="https://arxiv.org/abs/2212.08751">PointE</a></td> </tr> </tbody> </table> <h2 id="3d-representation">3d representation</h2> <p>As pictures have several approaches like SVG and PNG 3d models also have different representation.</p> <ul> <li>NERF</li> </ul> <p>Given the camera position $\mathbf{o}$ and direction $\mathbf{d}$, a batch of rays $\mathbf{r}(k) = o + k\mathbf{d}$ is sampled to render a pixel. The MLP takes $r(k)$ as input and predicts the density $τ$ and color $c$.</p> <p>Final rendered color is given by quadrature:</p> \[C_c(r) = \sum^{N_c}_{i=1} \Omega_i(1-\exp(-\tau_i \delta_i))c_i\] <p>$\Omega$ denotes accumalated transmitance</p> \[\Omega_i = \exp(-\sum^{i-1}_{j=1} \tau_j \delta_j)\] <p>$\delta$ - is distance between adjacent samples.</p> <ul> <li>Textured Mesh</li> </ul> <p>Textured mesh [45] represents the geometry of a 3D object with triangle meshes and the texture with color on the mesh surface. Here the 3D parameter θ consists of the parameters to represent the coordinates of triangle meshes and parameters of the texture. The rendering process g(θ, c) given camera pose c is defined by casting rays from pixels and computing the intersections between rays and mesh surfaces to obtain the color of each pixel. The textured mesh allows high-resolution and fast rendering with differentiable rasterization.</p> <p>Current</p> <h2 id="rendering">Rendering</h2> <p>https://pytorch3d.org/</p> <p>Most crucial equation comes. It’s defined through density estimation</p> <h2 id="nerf">NERF</h2> <p>Awesome overview of NERF are presented in <a href="https://theaisummer.com/nerf/">AI SUMMER</a></p> <table> <thead> <tr> <th style="text-align: center"><img src="/assets/img/posts/three_d_dmodels/neural_field.png" alt="NERF.jpg"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>NERF</em></td> </tr> </tbody> </table> <h2 id="techniques">Techniques</h2> <ul> <li>Point Cloud</li> <li>Score Distillation Sampling</li> </ul> <p>I list influential</p> <h2 id="influential-works">Influential works</h2> <p>Google Research</p> <ul> <li>DreamFusion https://dreamfusion3d.github.io/</li> </ul> <h2 id="score-distalation-sampling">Score distalation Sampling</h2> <p>https://pals.ttic.edu/p/score-jacobian-chaining</p> <p>is a widely used method to distill 2D image priors from a pretrained diffusion model ϵϕ into differentiable 3D representations. Given a differentiable generator g and a NeRF model parameterized by θ, its rendered image x can be obtained by x = g(θ). Then, SDS calculates the gradients of NeRF parameters θ by:</p> \[\nabla_\Theta \mathcal{L}_{SDS}(\phi,\mathbf{s}) = \mathrm{E}_{t,\epsilon} \left [\omega_t (\epsilon_\phi(x_t;y,t) - \epsilon) \frac{\partial z_t}{\partial x} {\partial}\right]\] <p>$\omega_t$ is a weighting function that depends on the timestep $t$ and $y$ is the text embedding of given prompt.</p> <h2 id="latest-article">Latest article</h2> <p>x. SDS is an optimization method by distilling pretrained diffusion models,</p> <p>As Advice using</p> <h2 id="resourses">Resourses</h2> <p>In article ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation https://arxiv.org/pdf/2305.16213.pdf Wang et al.</p> <p>their impo</p> <p>https://github.com/yuanzhi-zhu/prolific_dreamer2d/tree/main</p> <h2 id="where-go-further">Where go further</h2> <h2 id="references">References</h2> <p>[1] Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation https://pals.ttic.edu/p/score-jacobian-chaining</p> <p>[2] DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation https://arxiv.org/pdf/2306.12422.pdf</p> <p>[3] v- ShapE https://arxiv.org/abs/2305.02463</p>]]></content><author><name></name></author></entry><entry><title type="html">Airflow Part2 teamwork organization</title><link href="https://nmashalov.github.io/blog/2024/airflow_team/" rel="alternate" type="text/html" title="Airflow Part2 teamwork organization"/><published>2024-02-21T00:00:00+00:00</published><updated>2024-02-21T00:00:00+00:00</updated><id>https://nmashalov.github.io/blog/2024/airflow_team</id><content type="html" xml:base="https://nmashalov.github.io/blog/2024/airflow_team/"><![CDATA[<p>Architecture is seeking for ability for delegating responsibilities</p> <h2 id="responsibility-delegating">Responsibility delegating</h2> <p>Business critical processes requires swift responses for change of production environments . Hence specialists should be able to effectively collaborate in critical situations.</p> <p>Commonly used technique for that is introducing role model for specialists with.</p> <h2 id="airflow-team">Airflow team</h2> <table> <thead> <tr> <th style="text-align: center"><img src="/assets/img/posts/airflow_automation/team.png" alt="team.jpg"/></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>Our team</em></td> </tr> </tbody> </table> <h2 id="analytic">Analytic</h2> <p>Analytic concentrates on novel ideas for models and bring/</p> <h2 id="engineer">Engineer</h2> <p>Engineer is mostly reponsible</p> <p>Prerequisites:</p> <ul> <li>proficient with bash</li> </ul> <p>Although he helps to relaxate climax situation. He helps analytic and discuss tests.</p> <h2 id="developer">Developer</h2> <p>Every specialist is essential and has an ability to master his skills.</p> <h2 id="team-interaction-example">Team interaction example</h2> <p>Will discuss</p> <p>###</p> <p>Number of calculations in job has grown through successive application in busssiness. Yet pandas dataframe isn’t best format for handling big data</p> <h2 id="goal-interaction">Goal interaction:</h2> <ul> <li>engineer modifies from bash</li> </ul> <p>So that they can be gracefully proceeds with analytic codes.</p> <h3 id="analytic-want-to-revert-or-introduce">Analytic want to revert or introduce</h3>]]></content><author><name></name></author><summary type="html"><![CDATA[Article covers responsibility delegation in theme]]></summary></entry><entry><title type="html">Statistical physics analyses Free ener</title><link href="https://nmashalov.github.io/blog/2024/free_energy/" rel="alternate" type="text/html" title="Statistical physics analyses Free ener"/><published>2024-02-21T00:00:00+00:00</published><updated>2024-02-21T00:00:00+00:00</updated><id>https://nmashalov.github.io/blog/2024/free_energy</id><content type="html" xml:base="https://nmashalov.github.io/blog/2024/free_energy/"><![CDATA[\[p(E) = \frac{exp()}{Z}\] <p>$Z$ - partition function .,\</p> <h2 id="futher-read">Futher read</h2>]]></content><author><name></name></author><summary type="html"><![CDATA[Energy based approaches provide flexible framework of sta]]></summary></entry><entry><title type="html">Phase Transition in Neural Nets</title><link href="https://nmashalov.github.io/blog/2024/nn_phase_transition/" rel="alternate" type="text/html" title="Phase Transition in Neural Nets"/><published>2024-02-21T00:00:00+00:00</published><updated>2024-02-21T00:00:00+00:00</updated><id>https://nmashalov.github.io/blog/2024/nn_phase_transition</id><content type="html" xml:base="https://nmashalov.github.io/blog/2024/nn_phase_transition/"><![CDATA[<h2 id="analitic-solution-of-attention">Analitic solution of attention</h2> <p>A phase transition between positional and semantic learning in a solvable model of dot-product attention https://arxiv.org/abs/2402.03902</p> <p>Article advices solution of problem and shows phase transition</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Analitic solution of attention]]></summary></entry><entry><title type="html">Theoretical physics Part 2 Phase Transition in Neural Nets</title><link href="https://nmashalov.github.io/blog/2024/renormalization/" rel="alternate" type="text/html" title="Theoretical physics Part 2 Phase Transition in Neural Nets"/><published>2024-02-21T00:00:00+00:00</published><updated>2024-02-21T00:00:00+00:00</updated><id>https://nmashalov.github.io/blog/2024/renormalization</id><content type="html" xml:base="https://nmashalov.github.io/blog/2024/renormalization/"><![CDATA[<h2 id="ising-model">Ising model</h2> <h2 id="landau-theory-of-phase-transition">Landau theory of phase transition</h2> <p>Main intuition is that radical changes of matter is connected with change of it’s energetic</p> \[H(X)\] <p>$\Lambda$ is known is order parameter</p> <p>Cogerence length</p> \[\] <h2 id="futher-read">Futher read</h2> <ul> <li>Percolation: a Mathematical Phase Transition https://www.youtube.com/watch?v=a-767WnbaCQ</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[Ising model]]></summary></entry><entry><title type="html">Ru Nougat</title><link href="https://nmashalov.github.io/blog/2024/ru-nougat/" rel="alternate" type="text/html" title="Ru Nougat"/><published>2024-02-21T00:00:00+00:00</published><updated>2024-02-21T00:00:00+00:00</updated><id>https://nmashalov.github.io/blog/2024/ru-nougat</id><content type="html" xml:base="https://nmashalov.github.io/blog/2024/ru-nougat/"><![CDATA[<p>Nougat is</p> <p>https://arxiv.org/abs/2308.13418</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Nougat is]]></summary></entry><entry><title type="html">Interview</title><link href="https://nmashalov.github.io/blog/2024/interview/" rel="alternate" type="text/html" title="Interview"/><published>2024-01-16T00:00:00+00:00</published><updated>2024-01-16T00:00:00+00:00</updated><id>https://nmashalov.github.io/blog/2024/interview</id><content type="html" xml:base="https://nmashalov.github.io/blog/2024/interview/"><![CDATA[<p>My interests are generative modeling based on physics and geometrical methods.</p> <p>This is primarily determined by recent advances in generative modeling. Specifically, framework of diffusion networks consisting of closely related markov chains, stochastic ode and langevin dynamics. I believe that solution for coherence of representation of long can be found via analysis through fundamental physics law of continuation and motion.</p> <p>Yandex has rich access to media data. computational resources and experience of developing diffusion networks as YandexART. I am looking for a guidance in writing top papers proposing new approaches.</p> <p>Dream Fusion: Text-to-3D using 2D Diffusion - Preprint https://arxiv.org/abs/2209.14988</p> <p>Artictle proposed a novel approach to text-to-3D synthesis by leveraging a pretrained 2D text-to-image diffusion model. Crucially, the proposed approach eliminates the need for labeled 3D training data and avoids modifications to the image diffusion model. The optimization involves refining a randomly-initialized 3D mode ) through gradient descent, ensuring low loss in its 2D renderings from random angles.</p> <hr/> <p>Profilic dreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation- https://arxiv.org/abs/2305.16213</p> <p>Authors proposed novel approach for distillation 2d diffusion text-to-image models to 3d. They elaborate previous technique SDS introduced in Dream Fusion(https://dreamfusion3d.github.io/) by building distribution of implicit representation rather making it constant. For facilitating computation they utilize particle-based variational inference. I get of method intuition through paper Understanding and Accelerating Particle-Based Variational Inference(https://arxiv.org/pdf/1807.01750.pdf). Despite time consuming inference fidelity approach is much better than competing works. Moreover model enjoy appropriate for diffusion models guidance scale around 10.</p> <p>Action Matching: Learning Stochastic Dynamics from Samples -https://arxiv.org/abs/2210.06662</p> <p>Action Matching addresses the challenge of learning the continuous dynamics of a system when only provided with snapshots of its temporal marginals. I believe that such approaches will help to build semantics of complex actions like.</p> <hr/> <p>I gained great experience by building infrastructures for ML models, which can be easily modified via configs and command line interface. As for code preferences I prefer laconic style and enjoy usage of generator expressions where it possible. When possible I use task specific libraries as it helps concentrate on research task rather.</p> <p>Here’s representative example of ordering directed graph written in JSON</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># link has format [1,1,0,2,1,'.csv']
# link[0] - id of link (order of bringing links to folder)
# link[1] - id of source node
# link[3] - id of target node
# node.outputs contain use link id
# that's why we need link_map
# you can have better understanding looking at graph.json in test folder
</span>
<span class="c1"># Create a mapping of link IDs to their corresponding links
</span><span class="n">link_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">link</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">link</span> <span class="k">for</span> <span class="n">link</span> <span class="ow">in</span> <span class="n">links</span><span class="p">}</span>

<span class="c1"># Define a function to extract relevant IDs from a link
</span><span class="n">extract_ids</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">link_map</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="mi">1</span><span class="p">],</span> <span class="n">link_map</span><span class="p">[</span><span class="n">x</span><span class="p">][</span><span class="mi">3</span><span class="p">],)</span>

<span class="c1"># Create a dictionary mapping node links to their corresponding IDs
</span><span class="n">linkage</span> <span class="o">=</span> <span class="p">{</span>
    <span class="nb">input</span><span class="p">.</span><span class="n">link</span><span class="p">:</span> <span class="nf">extract_ids</span><span class="p">(</span><span class="nb">input</span><span class="p">.</span><span class="n">link</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">nodes</span>
    <span class="k">if</span> <span class="n">node</span><span class="p">.</span><span class="n">inputs</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">node</span><span class="p">.</span><span class="n">inputs</span>
    <span class="k">if</span> <span class="nb">input</span><span class="p">.</span><span class="n">link</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
<span class="p">}</span>

<span class="c1"># Build a directed graph using NetworkX
</span><span class="n">dependency_graph</span> <span class="o">=</span> <span class="n">nx</span><span class="p">.</span><span class="nc">DiGraph</span><span class="p">(</span><span class="n">linkage</span><span class="p">.</span><span class="nf">values</span><span class="p">())</span>

<span class="c1"># Group the generations of the DAG
</span><span class="n">grouped_dag</span> <span class="o">=</span> <span class="p">[</span><span class="nf">sorted</span><span class="p">(</span><span class="n">generation</span><span class="p">)</span> <span class="k">for</span> <span class="n">generation</span> <span class="ow">in</span> <span class="n">nx</span><span class="p">.</span><span class="nf">topological_generations</span><span class="p">(</span><span class="n">dependency_graph</span><span class="p">)]</span>
</code></pre></div></div> <p>Utilization of the “Don’t Repeat Yourself” (DRY) principle helped me a lot with learning benefits of abstractions, decomposing and code refactoring. You can see my progress in developing In my project multi-component projects here: https://github.com/NMashalov/PydanticGraph. In PydanticGraph i worked a lot with validation framework Pydantic. That brought me intuition of working with developer abstractions and python inner libraries like importlib.</p> <p>SDEdit: Image synthesis and editing with stochastic differential equations. a</p>]]></content><author><name></name></author><summary type="html"><![CDATA[My interests are generative modeling based on physics and geometrical methods.]]></summary></entry><entry><title type="html">Stein Variational Gradient Descent</title><link href="https://nmashalov.github.io/blog/2024/SVGD/" rel="alternate" type="text/html" title="Stein Variational Gradient Descent"/><published>2024-01-15T00:00:00+00:00</published><updated>2024-01-15T00:00:00+00:00</updated><id>https://nmashalov.github.io/blog/2024/SVGD</id><content type="html" xml:base="https://nmashalov.github.io/blog/2024/SVGD/"><![CDATA[<p>Stein Variational Gradient Descent uses a vector field $v$ to sample from prior distribution $\pi$ from $\rho$.</p> \[x^i_{k+1} = x_k^i + \varepsilon v(x_k^i)\] <p>For successful approximation $v$ is optimized via rate</p> \[\arg \sup_{\phi \in C}\{-\partial_\varepsilon KL(T_{\#\rho} \parallel \pi)_{\varepsilon=0}\}\] <p>Note that hedge $#$ hear means push forward operator from measure $\rho$. It’s just a way to emphasize that method is based on operator $T$</p> <p>However, directly computing the vector field is intractable because it is non-trivial to compute the time-dependent score function ∇θ log µτ (θ). To tackle this issue, traditional ParVI methods either restrict the functional gradient within RKHS and leverage analytical kernels to approximate the vector field or , or learn a neural network to estimate the vector field</p> <h2 id="wasserstein-gradient-flow">Wasserstein Gradient Flow</h2> <p>Large-Scale Wasserstein Gradient Flows https://arxiv.org/pdf/2106.00736.pdf. Is a method based on the Wasserstein gradient flows, that was proposed for solution of the Fokker-Planck equation.</p> <p>The term on the right can be understood as the gradient of F in Wasserstein space, a vector field perturbatively rearranging the mass in ρt to yield the steepest possible local change of F. Wasserstein gradient flows are used in various applications:</p> \[\frac{\partial \rho_t}{\partial t} = \text{div}(\rho_t \nabla_x \mathbf{F}(\rho_t))\] <p>Continuity equation</p> <p>Fokker-Plank free energy functional</p> \[F_{FP}(\rho)= U(\rho) - \beta^{-1} \mathcal{E}_\rho\] <h2 id="stein-operator">Stein operator</h2> \[S_\pi \phi = \nabla \log \pi \phi + \nabla \dot \phi\] <p>We need to find field $\phi$ that will transform prior distribution to posterior $\pi$</p> <h2 id="probability-metric">Probability metric</h2> <p>https://www.cs.toronto.edu/tss/files/papers/2021-SteinsMethodSurvey-Li.pdf</p> <p>Definition. Probability measures $\mu$ and $\nu$ on familiy $\mathbf{H}$ of test functions</p> \[d_{\mathbf{H}}(\mu, \nu) = \sup_{h \in \mathbf{H}} \left| \int h(x)d\mu(x) - \int g(x) d\nu(x)\right|\] <p>Wasserstein metric</p> <p>Family is defined via 1-Lipshitz functions $W$</p> \[d_W = \sup_{h\in W}|\mathrm{E}h(y)-\mathrm{E}h(Z)|\] <p>Stein identity</p> \[E f'(Z) = EZ f(Z)\] <table> <tbody> <tr> <td>for all absolute continious functions $f: \mathrm{R} \rightarrow \mathrm{R}$ with $\mathrm{E}</td> <td>f’(Z)</td> <td>&lt; \infty$</td> </tr> </tbody> </table> <h2 id="mean-field">Mean-field</h2> \[\frac{d X^i_t}{d t} = - \underbrace{\frac{1}{N} \sum_{j=1}^N \nabla k (X^i_t, X^j_t)}_{repulsion between particles} - \frac{1}{N}\] <h2 id="wasserstein-space">Wasserstein space</h2> <p>Denote $L^2_q$ as Hilbert Space of $\mathrm{R}^D$ valued functions with :</p> <ol> <li>u: $\mathrm{R^D} \rightarrow \mathrm{R^D} $ \(|\int\|u(x)\|^2_2 dq &lt; \infty|\)</li> <li>Inner product \(&lt;u,v&gt;_{L^2_q} = \int u(x) \dot v(x) dq\)</li> </ol> <p>\(\partial_t q_t + \nabla \dot (v_tq_t)=0,\) $v_t \in \bar{{\nabla \phi: \phi \in C_c^\infty}}$</p> <h2 id="gradient-flows">Gradient flows</h2> <p>Langevin and SGVD are gradient flows of KL divergence but with different metrics on $P(\mathbf{R}^d)$</p> <p>Langevin gradient flow approximation</p> \[\rho_{n+1} = \argmin_{\rho}(KL(\rho|\pi) + \frac{1}{\varepsilon} d^2_{OT}(\rho,\rho_n))\] <p>SGVD gradient flow approximation</p> \[\rho_{n+1} = \argmin_{\rho}(KL(\rho|\pi) + \frac{1}{\varepsilon} d^2_{K}(\rho,\rho_n))\] <p>Wasserstein geometry</p> <h2 id="particle-based-variational-inference">Particle-based Variational Inference</h2> <p>Typically, the measure µτ follows the “steepest descending curves” of a functional on W2(Θ),</p> <h2 id="practical-usage-of-svgd">Practical usage of SVGD</h2> <p><a href="https://arxiv.org/pdf/2305.16213.pdf">Profilic dreamer</a> - text-to-3D generation.</p> <p>Similarity between</p> <p>Compared with the vanilla SDS in Eq. (2) which optimizes the parameter θ in the parameter space Θ, here we aim to optimize the distribution (measure) µ(θ|y) in the function space W2(Θ).</p> \[\min_{\mu \in W_2(\Theta)} \varepsilon[\mu] = \mathrm{E}_{t,\varepsilon,c} [\frac{\sigma_t}{\alpha_T} \omega(t) D_{KL}(q_t^\mu(x_t|c,y) \parallel p_t(x_t|y^c))]\] <p>They derive the gradient flow minimizing $\mathcal{E}[\mu]$ in $\mathrm{W}_2(\Theta)$ and so update rule</p> \[\frac{\partial \mu_\tau(\theta|y)}{\partial \tau} = - \nabla_\theta \left[\mu_\tau(\theta|y) \mathrm{E}_{t,\epsilon,c}[\sigma_t \omega(t)(\nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t|y^c)- \nabla_{x_t} \log q_t^{\mu_t}(\mathbf{x}_t|c,y) \frac{\mathbf{g}(\theta,c)}{\partial \theta})] \right]\] <p>update rule</p> \[\frac{d \theta_\tau}{d \tau} = \mathrm{E}_{t,\epsilon,c}[\sigma_t \omega(t)(\nabla_{\mathbf{x}_t} \log p_t(\mathbf{x}_t|y^c)- \nabla_{x_t} \log q_t^{\mu_t}(\mathbf{x}_t|c,y) \frac{\mathbf{g}(\theta,c)}{\partial \theta})]\] <p>Theorem 3 shows that by letting the random variable θτ ∼ µτ (θτ |y) move across the ODE trajectory in Eq. (12), its underlying distribution µτ will move by the direction of the steepest descent that minimizes E[µ].</p> <p>Therefore, to obtain samples (in Θ) from µ ∗ = arg minµ E[µ], we can simulate the ODE in Eq. (12) by estimating two score functions ∇xt log pt(xt|y c ) and ∇xt log q µτ t (xt|c, y) at each ODE time τ , which corresponds to the VSD objective in Eq. (9).</p> <p>Proof. Hope it will help you acquire habbit:</p> <p>Gradient flow minimizing $\mathcal{E}[\mu]$ on $\mathrm{W}_2(\Theta)$ satisfies:</p> \[\frac{\partial\mu_\tau}{\partial \tau} = - \nabla_{\mathrm{W}_2} \mathcal{E}[\mu] = \nabla_\theta (\mu_t \nabla_\theta \frac{\delta \mathcal{E}[\mu_t]}{\delta \mu_t})\] <p>Calculating derivative</p> \[(\frac{\delta D_{KL}(q \parallel p)}{\delta q})[x] = \log q(x) - \log p(x) + 1\] \[\frac{\delta q_t^\mu(\mathbf{x}_t|c,y)}{\delta \mu}[\theta] = q_{t0}(\mathbf{x_t}|\mathbf{x_0}) = \mathcal{N}(\mathbf{x}_t| \alpha_tx_0)\] <p>By definition of $q_t^\mu$</p> \[q_t^\mu(\mathbf{x_t}|c,y) = \mathrm{E}_{q_0^\mu(\mathbf{x}_0|c,y)}[q_{t0}(\mathbf{x}_t|\mathbf{x}_0)] = \mathrm{\mu(\theta|y)}[q_{t0}(\mathbf{x_t}|\mathbf{g}(\theta,c))]\] <p>Fokker-Plank equation helps to connect measure equation with particle.</p> <p>References</p> <ol> <li>Andrew Duncan – On the Geometry of Stein Variational Gradient Descent https://www.youtube.com/watch?v=1Ec7Eoa5W7g</li> <li>Understanding and Accelerating Particle-Based Variational Inference https://arxiv.org/pdf/1807.01750.pdf</li> <li>B. T. Polyak, Some methods of speeding up the convergence of iteration methods https://www.mathnet.ru/links/e4418385804fdfbc633dced14b9713ec/zvmmf7713.pdf</li> </ol>]]></content><author><name></name></author><summary type="html"><![CDATA[Stein Variational Gradient Descent uses a vector field $v$ to sample from prior distribution $\pi$ from $\rho$.]]></summary></entry><entry><title type="html">Information geometry</title><link href="https://nmashalov.github.io/blog/2024/information-geometry/" rel="alternate" type="text/html" title="Information geometry"/><published>2024-01-15T00:00:00+00:00</published><updated>2024-01-15T00:00:00+00:00</updated><id>https://nmashalov.github.io/blog/2024/information-geometry</id><content type="html" xml:base="https://nmashalov.github.io/blog/2024/information-geometry/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Fisher information metric</p> \[\text{KL}(f_\theta|f_{\theta+d\theta}) = \frac{1}{2} (d\theta)^TI(\theta)d\theta +O(|d\theta|^3)\] <p>Fisher information</p> \[I(\theta) = \mathbb{E}_\theta[s_\theta(X)s_\theta(X)^T], s_\theta = \nabla_\theta \log f_\theta(X)\] <h2 id="riemannian-manifold">Riemannian manifold</h2> <p>Fisher information is symmetric positive semi-definite. Defines Riemannian metric on the parameter space(RAO, 1945) - Fisher information metric:</p> \[&lt;d\theta_1, d\theta_2&gt;_\theta = d\theta_1^T I(\theta6) d\theta_2\] <p>So for tangent vector $u,v \in T_\theta \Theta \approx \mathbb{R}^d$</p> \[&lt;u,v&gt;_\theta = u^T I(\theta)\] <h2 id="riemanian-tools">Riemanian tools</h2> <h3 id="angles-and-norms-of-tangent-vectors">Angles and norms of tangent vectors</h3> \[&lt;u,v&gt;_\theta\] <h3 id="lengths">Lengths</h3> \[l(\gamma) = \int_0^1 \|\dot{\gamma}(t)\|_{\gamma(t)}dt\] <h3 id="distances">Distances</h3> \[\text{dist}(\theta_0,\theta_1) = \inf_{\gamma(0)=\theta_0, \gamma(1)=\theta_1} l(\gamma)\] <h3 id="geodesics">Geodesics</h3> <p>Optimal interpolation with zero acceleration</p> \[t \rightarrow \gamma(t), \nabla_{\dot{\gamma}} \dot{\gamma} =0\] <p>$\nabla$ Levi-Civitia connections -&gt; intrisic derivatives of vector fields</p> <h2 id="hopf-rinov-theorem">Hopf-Rinov theorem</h2> <p>If manifold is complete, than any two points can be linked with minimizing geodesics</p> <h2 id="exp-map-and-log-map">Exp map and log map</h2> \[\forall \nu \in T_{\theta_0} \Theta, \exp_{\theta_)}(\nu)=\gamma(1), \text{where } \gamma \text{ is geodesic } \gamma(0)=\theta_0, \dot{\gamma}(0)=\nu\] \[\forall \theta_1 \in \Theta \log_{\theta_0} (\theta_1) = \nu, \text{ where} \exp_{\theta_0}(\nu)=\theta_1\] <h2 id="dual-tools">Dual tools</h2> <ul> <li>Eucledian scalar product $\rightarrow$ Riemannian metric</li> <li>straight lines $\rightarrow$ geodesics</li> <li>Euclidian distance $\rightarrow$ geodesic distance</li> <li>addition/ substraction $\rightarrow$ Riemannian exponential/logarithm</li> </ul> \[x + \nu \rightarrow \exp_x(\nu) \\ x - y \rightarrow \log_x(y)\] <h2 id="freshett-mea">Freshett mea</h2> <p>$x_1,\dots, x_n \in M$ metric space:</p> \[\tilde{x} =\argmin_{x\in M} \frac{1}{n} \sum_{i=1}^n d(x,x_i)^2\] <p>https://www.youtube.com/watch?v=elSmfwHNTRc</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Airflow Part 1 Architecture</title><link href="https://nmashalov.github.io/blog/2024/airflow_arch/" rel="alternate" type="text/html" title="Airflow Part 1 Architecture"/><published>2024-01-10T00:00:00+00:00</published><updated>2024-01-10T00:00:00+00:00</updated><id>https://nmashalov.github.io/blog/2024/airflow_arch</id><content type="html" xml:base="https://nmashalov.github.io/blog/2024/airflow_arch/"><![CDATA[<p>Through my previous year I had intensive work as MLOps specialist. One of my task was to facilitate in-house Airflow provision. I’ll share some techniques, that I implemented during my work.</p> <h2 id="airflow">Airflow</h2> <p>Apache Airflow is an open-source platform designed to programmatically author, schedule, and monitor workflows. It allows users to define complex workflows as Directed Acyclic Graphs (DAGs), where each node in the graph represents a task, and the edges define the dependencies between tasks. Airflow provides a rich set of features, including a web-based user interface for DAG management, integration with various data sources and processing engines, and robust scheduling capabilities. Users can easily create, schedule, and monitor workflows using Python scripts, leveraging a vast ecosystem of pre-built operators for common tasks such as executing SQL queries, transferring files, or running Python functions. Airflow’s extensible architecture allows for the integration of custom operators and hooks, enabling seamless orchestration of workflows across different systems. With its emphasis on flexibility, scalability, and reliability, Apache Airflow has become a popular choice for orchestrating data pipelines, ETL processes, machine learning workflows, and more in a wide range of industries and use cases.</p> <h2 id="setup">Setup</h2> <p>Airflow organization was build for necessities of classical ML team, which specialized on test and making hyposthesis. Therefore following objectives were established:</p> <ul> <li> <h2 id="transparency">transparency:</h2> </li> <li></li> </ul> <h2 id="one-operator-many-possibilitites">One operator many possibilitites</h2> <p>Operators</p> <p>Which were configured as</p> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">job</span><span class="pi">:</span>
    <span class="na">resourse_flavor</span><span class="pi">:</span> <span class="c1"># configures amount of cpu, gpu and ram</span>
    <span class="na">docker_image</span><span class="pi">:</span> <span class="c1"># sets image </span>


</code></pre></div></div> <p>##</p> <h2 id="templating-helps">Templating helps</h2> <p>Dag were created</p> <p>Through use of Jinja</p> <p>I provide you with my template to start</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>


</code></pre></div></div> <h2 id="ci-delivery">CI delivery</h2> <p><img src="/assets/img/posts/drawing/Simo-Serra/drawing.png" alt="drawing.jpg"/></p> <h2 id="environments-helps-to-test-new-ideas">Environments helps to test new ideas</h2> <p>We decoupled setup by different Airflow instances.</p> <p>Difference</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>config:
    

</code></pre></div></div> <h2 id="yaml">Yaml</h2> <p>Versioning</p> <h2 id="node-based-programming">Node-based programming</h2> <p>Finally</p> <p>I share my</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Article covers core architecture for beginner ml team.]]></summary></entry></feed>