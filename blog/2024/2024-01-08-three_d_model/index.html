<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction"> Introduction </a></div> <div><a href="#the-score-function-score-based-models-and-score-matching">The score function, score-based models, and score matching</a></div> <div><a href="#langevin-dynamics">Langevin dynamics</a></div> <div><a href="#naive-score-based-generative-modeling-and-its-pitfalls">Naive score-based generative modeling and its pitfalls</a></div> <div><a href="#score-based-generative-modeling-with-multiple-noise-perturbations">Score-based generative modeling with multiple noise perturbations</a></div> <div> <a href="#score-based-generative-modeling-with-stochastic-differential-equations-sdes">Score-based generative modeling with stochastic differential equations (SDEs)</a> </div> <ul> <li><a href="#perturbing-data-with-an-sde">Perturbing data with an SDE</a></li> <li><a href="#reversing-the-sde-for-sample-generation">Reversing the SDE for sample generation</a></li> <li><a href="#estimating-the-reverse-sde-with-score-based-models-and-score-matching">Estimating the reverse SDE with score-based models and score matching</a></li> <li><a href="#how-to-solve-the-reverse-sde"> How to solve the reverse SDE </a></li> <li><a href="#probability-flow-ode">Probability flow ODE</a></li> <li><a href="#controllable-generation-for-inverse-problem-solving">Controllable generation for inverse problem solving</a></li> </ul> <div><a href="#connection-to-diffusion-models-and-others">Connection to diffusion models and others</a></div> <div><a href="#concluding-remarks">Concluding remarks</a></div> </nav> </d-contents> <p>Here’s my overview of current achievements in 3D</p> <p>Formats can be transformed</p> <h2 id="two-main-approaches">Two main approaches</h2> <table> <thead> <tr> <th>Flavor</th> <th>Distillation from 2d images</th> <th>Work with 3d models</th> </tr> </thead> <tbody> <tr> <td>Data</td> <td>Cheap</td> <td>Expensive</td> </tr> <tr> <td>Speed</td> <td>currently slow</td> <td>fast</td> </tr> <tr> <td>Influential works</td> <td><a href="https://arxiv.org/pdf/2305.16213.pdf" rel="external nofollow noopener" target="_blank">Profilic Dreamer</a></td> <td><a href="https://arxiv.org/abs/2212.08751" rel="external nofollow noopener" target="_blank">PointE</a></td> </tr> </tbody> </table> <h2 id="3d-representation">3d representation</h2> <p>As pictures have several approaches like SVG and PNG 3d models also have different representation.</p> <ul> <li>NERF</li> </ul> <p>Given the camera position $\mathbf{o}$ and direction $\mathbf{d}$, a batch of rays $\mathbf{r}(k) = o + k\mathbf{d}$ is sampled to render a pixel. The MLP takes $r(k)$ as input and predicts the density $τ$ and color $c$.</p> <p>Final rendered color is given by quadrature:</p> \[C_c(r) = \sum^{N_c}_{i=1} \Omega_i(1-\exp(-\tau_i \delta_i))c_i\] <p>$\Omega$ denotes accumalated transmitance</p> \[\Omega_i = \exp(-\sum^{i-1}_{j=1} \tau_j \delta_j)\] <p>$\delta$ - is distance between adjacent samples.</p> <ul> <li>Textured Mesh</li> </ul> <p>Textured mesh [45] represents the geometry of a 3D object with triangle meshes and the texture with color on the mesh surface. Here the 3D parameter θ consists of the parameters to represent the coordinates of triangle meshes and parameters of the texture. The rendering process g(θ, c) given camera pose c is defined by casting rays from pixels and computing the intersections between rays and mesh surfaces to obtain the color of each pixel. The textured mesh allows high-resolution and fast rendering with differentiable rasterization.</p> <p>Current</p> <h2 id="rendering">Rendering</h2> <p>https://pytorch3d.org/</p> <p>Most crucial equation comes. It’s defined through density estimation</p> <h2 id="nerf">NERF</h2> <p>Awesome overview of NERF are presented in <a href="https://theaisummer.com/nerf/" rel="external nofollow noopener" target="_blank">AI SUMMER</a></p> <table> <thead> <tr> <th style="text-align: center"><img src="/assets/img/posts/three_d_dmodels/neural_field.png" alt="NERF.jpg"></th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><em>NERF</em></td> </tr> </tbody> </table> <h2 id="techniques">Techniques</h2> <ul> <li>Point Cloud</li> <li>Score Distillation Sampling</li> </ul> <p>I list influential</p> <h2 id="influential-works">Influential works</h2> <p>Google Research</p> <ul> <li>DreamFusion https://dreamfusion3d.github.io/</li> </ul> <h2 id="score-distalation-sampling">Score distalation Sampling</h2> <p>https://pals.ttic.edu/p/score-jacobian-chaining</p> <p>is a widely used method to distill 2D image priors from a pretrained diffusion model ϵϕ into differentiable 3D representations. Given a differentiable generator g and a NeRF model parameterized by θ, its rendered image x can be obtained by x = g(θ). Then, SDS calculates the gradients of NeRF parameters θ by:</p> \[\nabla_\Theta \mathcal{L}_{SDS}(\phi,\mathbf{s}) = \mathrm{E}_{t,\epsilon} \left [\omega_t (\epsilon_\phi(x_t;y,t) - \epsilon) \frac{\partial z_t}{\partial x} {\partial}\right]\] <p>$\omega_t$ is a weighting function that depends on the timestep $t$ and $y$ is the text embedding of given prompt.</p> <h2 id="latest-article">Latest article</h2> <p>x. SDS is an optimization method by distilling pretrained diffusion models,</p> <p>As Advice using</p> <h2 id="resourses">Resourses</h2> <p>In article ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score Distillation https://arxiv.org/pdf/2305.16213.pdf Wang et al.</p> <p>their impo</p> <p>https://github.com/yuanzhi-zhu/prolific_dreamer2d/tree/main</p> <h2 id="where-go-further">Where go further</h2> <h2 id="references">References</h2> <p>[1] Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D Generation https://pals.ttic.edu/p/score-jacobian-chaining</p> <p>[2] DreamTime: An Improved Optimization Strategy for Text-to-3D Content Creation https://arxiv.org/pdf/2306.12422.pdf</p> <p>[3] v- ShapE https://arxiv.org/abs/2305.02463</p> </body></html>