---
layout: post
title: Stein Variotional Gradient Descent
---


Stein Variational Gradient Descent uses a vector field $v$ to sample from prior distribution $\pi$ from $\rho$.

$$
    x^i_{k+1} = x_k^i + \varepsilon v(x_k^i)
$$

For successful approximation $v$ is optimized via rate

$$
    \arg \sup_{\phi \in C}\{-\partial_\varepsilon KL(T_{\#\rho} \parallel \pi)_{\varepsilon=0}\}
$$

Note that hedge $\#$ hear means pullback operator from distribution $\rho$. It's just a way to emphasize that method is based on operator $T$



 However, directly computing the vector field is intractable because
it is non-trivial to compute the time-dependent score function ∇θ log µτ (θ). To tackle this issue,
traditional ParVI methods either restrict the functional gradient within RKHS and leverage analytical
kernels to approximate the vector field or , or learn a neural network to estimate the vector
field

## Stein operator

$$
    S_\pi \phi = \nabla \log \pi \phi + \nabla \dot \phi 
$$


We need to find field $\phi$ that will transform prior distribution to posterior $\pi$ 


## Probability metric

https://www.cs.toronto.edu/tss/files/papers/2021-SteinsMethodSurvey-Li.pdf

Definition. Probability measures $\mu$ and $\nu$ on familiy $\mathbf{H}$ of test functions

$$
    d_{\mathbf{H}}(\mu,
    \nu) = \sup_{h \in \mathbf{H}} \left| \int h(x)d\mu(x) - \int g(x) d\nu(x)\right|
$$

Wasserstein metric

Family is defined via 1-Lipshitz functions $W$ 

$$
    d_W = \sup_{h\in W}|\mathrm{E}h(y)-\mathrm{E}h(Z)|
$$


Stein identity

$$
    E f'(Z) = EZ f(Z)
$$

for all absolute continious functions $f: \mathrm{R} \rightarrow \mathrm{R}$ with $\mathrm{E}|f'(Z)| < \infty$

## Mean-field

$$
    \frac{d X^i_t}{d t} = - \underbrace{\frac{1}{N} \sum_{j=1}^N \nabla k (X^i_t, X^j_t)}_{repulsion between particles} - \frac{1}{N}
$$

## Wasserstein space

Denote $L^2_q$ as Hilbert Space of $\mathrm{R}^D$ valued functions with :

1. 
    u: $\mathrm{R^D} \rightarrow \mathrm{R^D}  $
    $$
        |\int\|u(x)\|^2_2 dq < \infty|
    $$
2. Inner product
    $$
        <u,v>_{L^2_q} = \int u(x) \dot v(x) dq 
    $$

$$
    \partial_t q_t + \nabla \dot (v_tq_t)=0,
$$
$v_t \in \bar{\{\nabla \phi: \phi \in C_c^\infty\}}$

## Gradient flows

Langevin and SGVD are gradient flows of KL divergence but with different metrics on $P(\mathbf{R}^d)$

Langevin gradient flow approximation

$$
    \rho_{n+1} = \argmin_{\rho}(KL(\rho|\pi) + \frac{1}{\varepsilon} d^2_{OT}(\rho,\rho_n))
$$


SGVD gradient flow approximation


$$
    \rho_{n+1} = \argmin_{\rho}(KL(\rho|\pi) + \frac{1}{\varepsilon} d^2_{K}(\rho,\rho_n))
$$

Wasserstein geometry




## Practical usage of SVGD

[Profilic dreamer](https://arxiv.org/pdf/2305.16213.pdf) - text-to-3D generation.

Similarity between 



Compared with the vanilla SDS in Eq. (2) which optimizes the parameter θ in the parameter space Θ,
here we aim to optimize the distribution (measure) µ(θ|y) in the function space W2(Θ).

$$
    \min_{\mu \in W_2(\Theta)} \varepsilon[\mu] = \mathrm{E}_{t,\varepsilon,c}
 [\frac{\sigma_t}{\alpha_T} \omega(t) D_{KL}(q_t^\mu(x_t|c,y) \parallel p_t(x_t|y^c))]$$



References
1. Andrew Duncan – On the Geometry of Stein Variational Gradient Descent
https://www.youtube.com/watch?v=1Ec7Eoa5W7g
2. Understanding and Accelerating Particle-Based Variational Inference https://arxiv.org/pdf/1807.01750.pdf
3. B. T. Polyak, Some methods of speeding up the convergence
of iteration methods https://www.mathnet.ru/links/e4418385804fdfbc633dced14b9713ec/zvmmf7713.pdf