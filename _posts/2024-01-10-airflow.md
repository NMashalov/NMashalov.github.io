---
layout: distill
title: Airflow 
description: Article covers Airflow architecture for beginner enterprise ml team
authors:
  - name: Mashalov Nikita  
    affiliations: 
      name: MIPT
---
<d-contents>

  <nav class="l-text figcaption">
  <h3>Contents</h3>
    <div><a href="#tools"> Tools </a></div>  
    <div><a href="#team"> Team </a></div> 
    <div><a href="#code implementation">Code implementation</a></div> 
  </nav>
</d-contents>


Through year I had intensive work with Airflow as MLOps specialist. My primer task was to facilitate in-house Airflow provision. I'll share ideas and setup, that  

Blog divided in three parts:
1. Linkage between tools of versioning Gitlab, storage s3 and Airflow
2. Covers responsibility delegation in team
3. Suggest code for introduced concepts


If you prefer, you can material on [youtube](WIP) or look through [presentation](https://docs.google.com/presentation/d/1ZDHUNOikAwtRWcU3ctK962KErVaah2iQ4PrRD5tZjr8/edit?usp=sharing)

# Tools

Section will provide setup explanation and 

Before we start I'll share some core concepts about Airflow. 

## Airflow. Brief introduction


Apache Airflow is an open-source platform designed to programmatically compose pipeline, schedule them and monitor them in powerful UI. Main language is Python 

Basic primitive is a Directed Acyclic Graphs (DAGs). 

| ![dag.jpg](/assets/img/posts/airflow_automation/tutorial/dag.excalidraw.png) | 
|:--:| 
| *Simple representation of pipeline* |

Nodes here represents action to executed, links it's order.

That abstraction is boosted with scheduling

For better perception can be organized in groups.

| ![groups.jpg](/assets/img/posts/airflow_automation/tutorial/groups.excalidraw.png) | 
|:--:| 
| *Semantic organization* |


By default operators are executed via [Celery](https://docs.celeryq.dev/), but can also be modified for running on [Kubernetes](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/operators.html).

You can learn more about Airflow from official documentation on [DAG](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html). If you want to practice, you can also refer to my tutorial on starting with Airflow on toy OCR app [Github Link](https://github.com/NMashalov/Airflow_tutorial).

I'll warn you that from outbox Airflow:
- doesn't provide any options for . 

## Setup

In-house solution already done all all DevOps work of the deployment, configuration, and monitoring of Airflow instances. Provider ensured scalability, reliability, and performance.

Pipelines are provided to Airflow via cli command, which can be executed both locally and 


| ![s3_airflow.png](/assets/img/posts/airflow_automation/architecture/airflow_delivery.excalidraw.png) | 
|:--:| 
| *Our dream team* |


Yet it comes with restriction of  Airflow environments, manage dependencies, which could be mitigated with building custom airflow docker image.


Moreover, it introduced convenient way of job execution defined by yaml. It's api is quite transparent, I'll provide you comments to 

```yaml
job:
    resourse_flavor: cpu-16gb # configures amount of cpu, gpu and ram
    docker_image: python3-11 # sets image 
    excecution_time: 60 minutes # time of execution. After job ungracefully shuts
    execution_script: python main.py # defines script that will be executed through job
```

And a way of handling big files from executions through s3

| ![s3_airflow.png](/assets/img/posts/airflow_automation/architecture/handling_files.excalidraw.png) | 
|:--:| 
| *Transportation as a service* |


Therefore introduced techniques are primarly oriented for effective utilizations of provided features and 
effective ML team collaboration.

## Problems

Main problem were:
- testing models inference and node functionality was extremely on server
- monitoring of dags execution and data delivery
- environment setting
- no reasonable solution for reuse of already prepared dag

I'll introduce solution for every problem through next paragraphs

## CLI apps for testing

Cli apps should facilitate testing 

When app is ready we can have following options
- push to Python package storage, from where we'll pip install it in execution job
- make a docker image with install cli option


| ![s3_airflow.png](/assets/img/posts/airflow_automation/architecture/upload.excalidraw.png) | 
|:--:| 
| *Our dream team* |

Also repo contains reproduction code for model. Reproduction is done
with usage 

| ![s3_airflow.png](/assets/img/posts/airflow_automation/architecture/model.excalidraw.png) | 
|:--:| 
| *Training is performed in job, that is called in CI script. Than model proceeds to designated bucket* |


Every option is actually great and should be facilitated for usage 

## Pipeline for repetitive code

Main observation was that dags are mostly similar and has structure of Extract Transform Load(ETL).

Therefore they can be replaced with simple yaml structure as:
```yaml
dag: # dag info
    name: 
    schedule: " " # simple cron expression


```

## CI/CD for environment


Before we can start inference we need to deliver our files to production. Our provides can grab files for inference from s3. 

Repository of airflow and model were separated









### Operators










## One operator many possibilitites






# Team

Architecture is seeking for ability for delegating responsibilities 



## Responsibility delegating

Business critical processes requires swift responses for change of production environments . Hence specialists should be able to effectively collaborate in critical situations.

Commonly used technique for that is introducing role model for specialists with. 

## Airflow team

| ![team.jpg](/assets/img/posts/airflow_automation/team.excalidraw.png) | 
|:--:| 
| *Our dream team* |

## Data Scientist



Analytic concentrates on novel ideas for models and bring/


| ![ds.jpg](/assets/img/posts/airflow_automation/roles/ds/ds.excalidraw.png) | 
|:--:| 
| *Provider of novel ideas * |


Growth:

| ![ds_growth.jpg](/assets/img/posts/airflow_automation/roles/ds/scale.excalidraw.png) | 
|:--:| 
| *Stronger algorithm generalization * |


## Data engineer 

He knows bases of devops, yet specify his skills in building robust and flexible pipelines.

Engineer is mostly responsible for:
- help of analytics with optimal pipeline solution
- development of new operator for connection


| ![team.jpg](/assets/img/posts/airflow_automation/roles/engineer/engineer.excalidraw.png) | 
|:--:| 
| *Builds functional operator, scales code* |

Prerequisites:
- proficient with bash

Main track of learning is providing more flexible solutions. It's includes
- gluing code with bash
- 

| ![team.jpg](/assets/img/posts/airflow_automation/roles/engineer/growth.excalidraw.png) | 
|:--:| 
| *Builds functional operator, scales code* |


Although he helps to relaxate climax situation. He helps analytic and discuss tests.

## Developer

Mostly responsible for scalability, therefore ease of building new dags and their monitoring.

| ![developer.jpg](/assets/img/posts/airflow_automation/roles/developer/developer.excalidraw.png) | 
|:--:| 
| *Scales and rescales * |

Every specialist is essential and has an ability to master his skills.



| ![developer.jpg](/assets/img/posts/airflow_automation/roles/developer/growh.excalidraw.png) | 
|:--:| 
| *Developer learns scaling * |

## Team interaction

Will discuss

### Big csv

Suppose number of calculations in job has grown through successive application in bussiness. Yet pandas dataframe isn't best format for handling big data

| ![team.jpg](/assets/img/posts/airflow_automation/examples/big_csv.excalidraw.png) |
|:--:| 
| *Scaling can be painful for DS* |


Engineer can split csv before

| ![team.jpg](/assets/img/posts/airflow_automation/examples/big_csv_solution.excalidraw.png) |
|:--:| 
| *Branch and bound* |







Goal interaction:
-
- engineer modifies from bash 

So that they can be gracefully proceeds with analytic codes.


### Analytic want to revert or introduce




# Code implementation

Nitty details of implementation


## CI delivery


![drawing.jpg](/assets/img/posts/drawing/Simo-Serra/drawing.png)

## Environments helps to test new ideas

We decoupled setup by different Airflow instances. 


Difference 

```
config:
    

```




## s3 environment organization

Simple Storage Service (S3)  is a highly scalable, secure, and durable object storage. 

objects are organized and accessed within buckets.

Links in S3 are proximal to file storage





## Pipeline yaml structure

It easier to reason using groups

```yaml
dag: #meta info of dag
    n

jobs:


```

Then that code is templated in python dags


Yaml is useful for versioning of code and correction on fly from gitlab editor



Dag were created 

Through use of Jinja

I provide you with my template to start

```
```


###

### Node based programming


| ![ui.ppg](/assets/img/posts/airflow_automation/node_based/ui.png) | 
|:--:| 
| *Open source solution for class connection* |


For ease of building new dags I introduced [solution](https://github.com/NMashalov/PyVisGraph) based on [LiteGraph](https://github.com/jagenjo/litegraph.js/blob/master/build/litegraph.js). 

Solution was highly inspired by ComfyUI pipelining tool for Stable Diffusion.









