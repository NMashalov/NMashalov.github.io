---
layout: distill
title: Airflow 
description: >
  Article covers Airflow architecture for beginner enterprise ml team with use of   
---

Through my previous year I had intensive work as MLOps specialist. One of my task was to facilitate in-house Airflow provision. I'll share some techniques, that I implemented during my work.

Blog divided in three parts:
1. Linkage between tools of versioning Gitlab, storage s3 and Airflow
2. Covers responsibility delegation in team
3. Suggest code for introduced concepts


If you prefer, you can material on youtube
WIP

# Tools


## Airflow. Brief introduction


Apache Airflow is an open-source platform designed to programmatically author, schedule, and monitor workflows.

I advise you to start from official documentation on [DAG](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html). Yet I'll provide you quick into to build intuition for understanding article.

Basic primitive is a Directed Acyclic Graphs (DAGs). 

| ![dag.jpg](/assets/img/posts/airflow_automation/tutorial/dag.excalidraw.png) | 
|:--:| 
| *Simple representation of pipeline* |


Processing of information is done with operator nodes

Moreover, nodes for better perception can be organized in groups.

| ![groups.jpg](/assets/img/posts/airflow_automation/tutorial/groups.excalidraw.png) | 
|:--:| 
| *Semantic organization* |

You can refer to my tutorial to start Airflow with Docker on OCR app [Github Link](https://github.com/NMashalov/Airflow_tutorial).
## Setup

That means that all DevOps work of the deployment, configuration, and monitoring of Airflow instances. Provider ensured scalability, reliability, and performance.

Yet it comes with restriction of  Airflow environments, manage dependencies, which could be mitigated with building custom airflow docker image.

## Deployment 

Before we can start inference we need to deliver our files to production. Our provides can grab files for inference from s3. 

[]


## Pipeline 

Main observation was that dags are mostly similar and has structure of Extract Transform Load(ETL).

Therefore they can be replaced with simple yaml structure as:
```yaml
dag: # dag info
    name: 
    schedule: " " # simple cron expression


```


It a


### Operators









Airflow organization was build for necessities of classical ML team, which specialized on test and making hyposthesis. Therefore following objectives were 
established:

- transparency:  
-
- 

## One operator many possibilitites

Operators 

Which were configured as 


```yaml
job:
    resourse_flavor: # configures amount of cpu, gpu and ram
    docker_image: # sets image 


```


##







# Team

Architecture is seeking for ability for delegating responsibilities 



## Responsibility delegating

Business critical processes requires swift responses for change of production environments . Hence specialists should be able to effectively collaborate in critical situations.

Commonly used technique for that is introducing role model for specialists with. 

## Airflow team

| ![team.jpg](/assets/img/posts/airflow_automation/team.excalidraw.png) | 
|:--:| 
| *Our dream team* |

## Data Scientist



Analytic concentrates on novel ideas for models and bring/


| ![ds.jpg](/assets/img/posts/airflow_automation/roles/ds/ds.excalidraw.png) | 
|:--:| 
| *Provider of novel ideas * |


Growth:

| ![ds_growth.jpg](/assets/img/posts/airflow_automation/roles/ds/scale.excalidraw.png) | 
|:--:| 
| *Stronger algorithm generalization * |


## Data engineer 

He knows bases of devops, yet specify his skills in building robust and flexible pipelines.

Engineer is mostly responsible for:
- help of analytics with optimal pipeline solution
- development of new operator for connection


| ![team.jpg](/assets/img/posts/airflow_automation/roles/engineer/engineer.excalidraw.png) | 
|:--:| 
| *Builds functional operator, scales code* |

Prerequisites:
- proficient with bash

Main track of learning is providing more flexible solutions. It's includes
- gluing code with bash
- 

| ![team.jpg](/assets/img/posts/airflow_automation/roles/engineer/growth.excalidraw.png) | 
|:--:| 
| *Builds functional operator, scales code* |


Although he helps to relaxate climax situation. He helps analytic and discuss tests.

## Developer

Mostly responsible for scalability, therefore ease of building new dags and their monitoring.

| ![developer.jpg](/assets/img/posts/airflow_automation/roles/developer/developer.excalidraw.png) | 
|:--:| 
| *Scales and rescales * |

Every specialist is essential and has an ability to master his skills.



| ![developer.jpg](/assets/img/posts/airflow_automation/roles/developer/growh.excalidraw.png) | 
|:--:| 
| *Developer learns scaling * |

## Team interaction

Will discuss

### Big csv

Suppose we scaled our scoring pipeline


| ![team.jpg](/assets/img/posts/airflow_automation/examples/big_csv.excalidraw.png) |
|:--:| 
| *Scaling can be painful for DS* |


Engineer can split csv before

```bash
mlr --csv split -n 1000000 input.csv
```
| ![team.jpg](/assets/img/posts/airflow_automation/examples/big_csv_solution.excalidraw.png) |
|:--:| 
| *Branch and bound* |



Number of calculations in job has grown through successive application in busssiness. Yet pandas dataframe isn't best format for handling big data




Goal interaction:
-
- engineer modifies from bash 

So that they can be gracefully proceeds with analytic codes.


### Analytic want to revert or introduce




# Code implementation

Nitty details of implementation


## CI delivery


![drawing.jpg](/assets/img/posts/drawing/Simo-Serra/drawing.png)

## Environments helps to test new ideas

We decoupled setup by different Airflow instances. 


Difference 

```
config:
    

```




## s3 environment organization

Simple Storage Service (S3)  is a highly scalable, secure, and durable object storage. 

objects are organized and accessed within buckets.

Links in S3 are proximal to file storage





## Pipeline yaml structure

It easier to reason using groups

```yaml
dag: #meta info of dag
    n

jobs:


```

Then that code is templated in python dags


Yaml is useful for versioning of code and correction on fly from gitlab editor



Dag were created 

Through use of Jinja

I provide you with my template to start

```
```


###

### Node based programming


| ![ui.ppg](/assets/img/posts/airflow_automation/node_based/ui.png) | 
|:--:| 
| *Open source solution for class connection* |


For ease of building new dags I introduced [solution](https://github.com/NMashalov/PyVisGraph) based on [LiteGraph](https://github.com/jagenjo/litegraph.js/blob/master/build/litegraph.js). 

Solution was highly inspired by ComfyUI pipelining tool for Stable Diffusion.









